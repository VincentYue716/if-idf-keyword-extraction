{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% read a csv file and convert to string list\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_csv_column(csv_path, column_name):\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8', low_memory=False)\n",
    "    return df[column_name].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def extract_topn_from_vector(feature_name_list, sorted_item_list, topn):\n",
    "    \"\"\"\n",
    "    get the feature names and tf-idf score of top n items\n",
    "    \"\"\"\n",
    "\n",
    "    # use only topn items from vector\n",
    "    sorted_item_list = sorted_item_list[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_item_list:\n",
    "\n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_name_list[idx])\n",
    "\n",
    "    # create a tuples of feature,score\n",
    "    # results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# get the weibo contents\n",
    "weibo_contents = read_csv_column('./trump/preprocessed_v2.csv', '微博正文')\n",
    "\n",
    "# ignore words that appear in 85% of documents\n",
    "cv=CountVectorizer(max_df=0.85, max_features=10000)\n",
    "word_count_vector=cv.fit_transform(weibo_contents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% create a vocabulary of words\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "TfidfTransformer()"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% compute the IDF values\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "trump1_content = read_csv_column('./trump/trump_9.csv', '微博正文')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% computing TF-IDF and extracting keywords\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keywords\n",
      "美国 0.464\n",
      "中国 0.257\n",
      "政府 0.179\n",
      "总统 0.169\n",
      "9月 0.157\n",
      "拜登 0.147\n",
      "tiktok 0.125\n",
      "疫苗 0.106\n",
      "大选 0.104\n",
      "新冠 0.102\n",
      "报道 0.101\n",
      "疫情 0.101\n",
      "关税 0.095\n",
      "视频 0.094\n",
      "美元 0.093\n",
      "新闻 0.089\n",
      "死亡 0.082\n",
      "时间 0.08\n",
      "记者 0.08\n",
      "国家 0.078\n"
     ]
    }
   ],
   "source": [
    "item = ''\n",
    "for i in range(len(trump1_content)):\n",
    "    item = item + trump1_content[i]\n",
    "# item = trump1_content[1]\n",
    "# generate tf-idf for the given document\n",
    "tf_idf_vector = tfidf_transformer.transform(cv.transform([item]))\n",
    "\n",
    "# sort the tf-idf vectors by descending order of scores\n",
    "sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "# extract only the top n; n here is 10\n",
    "keywords = extract_topn_from_vector(feature_names, sorted_items, 20)\n",
    "\n",
    "# now print the results\n",
    "# print(\"\\n=====Doc=====\")\n",
    "# print(period_contents)\n",
    "print(\"\\nKeywords\")\n",
    "for k in keywords:\n",
    "    print(k, keywords[k])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}